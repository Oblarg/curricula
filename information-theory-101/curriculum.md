# Information Theory: How to Count the World

## Course Overview

This curriculum presents information theory as a unifying "meta-language" for science, using a discrete/combinatorial approach together with geometric intuition accessible to advanced high school students. It is designed for students with a strong mathematical background who are ready to explore fundamental questions about how complexity simplifies, how information flows, and how patterns emerge across diverse domains.

This curriculum does not assume familiarity with calculus or linear algebra.  Connections to these topics are made in the various "perspective" sections, and concepts that are necessary for understanding of the core ideas are introduced as needed (without deep technical development).  Familiarity with probability is required; the timing of the course is intended to allow simultaneous study with statistics, which will typically introduce the required notions of probability early in the year.

**Course Title:** Information Theory: How to Count the World

**Prerequisites:** 
- Precalculus (prior)
- Statistics or Probability (concurrent or prior)
 - **note:** The first half of Unit 1 is intended to be accessible to students with no prior exposure to probability, but the ideas develop at a rapid pace under the assumption that probability is being simultaneously/independently studied.  If a statistics course does *not* introduce probability early in the year, then it is not a suitable concurrent-prerequisite.

**Level:** Advanced High School Elective / Honors Course

**Course Duration:** Full year (36 weeks)

---

## Unit 1: The Geometry of Surprise

**Focus:** Develop the notion of "information" from the perspective of counting possibilities.

### Topics

- **Information as Counting Possibilities:** A heuristic discussion of what "information" means. Why does learning that a rare event occurred tell us more than learning a common event occurred? Developing the intuition that information should be inversely related to probability, and that we need a way to count the "number of possibilities" a system can be in.

- **Multiplicative Interaction of Probabilities:** When events are independent, their joint probability multiplies: P(A and B) = P(A) × P(B). Exploring concrete examples (coin flips, dice rolls, DNA sequences) to see how the number of possible outcomes grows multiplicatively.

- **The Notion of "Expected Probability":** When we have a distribution with multiple outcomes, we want to characterize it by some kind of "typical" or "expected" probability. The arithmetic mean of probabilities doesn't capture the multiplicative nature of independent events. Instead, we need the geometric mean: for probabilities p₁, p₂, ..., pₙ, the geometric mean is (p₁ × p₂ × ... × pₙ)^(1/n). This is the natural "expected" value for multiplicative quantities.
  - **Pedagogical note:** Here we establish the geometric mean as the natural structure for multiplicative quantities. Later in this unit, we'll flip this perspective: after transforming to additive space via the logarithm, we'll see how operations in that additive space (arithmetic means, mixing) relate back to this geometric structure.

- **The Logarithm as a Transformation:** The geometric mean is unwieldy; most ready-made mathematical "machinery" is designed to work with arithmetic means. The logarithm transforms geometric means into arithmetic means: log(geometric mean of probabilities) = arithmetic mean of log(probabilities). This is the key insight: by taking logarithms, we move from a multiplicative space (probabilities) to an additive space (information), where arithmetic means make sense. Recognizing that if information is additive (the information from two independent events should add), but probabilities multiply, the logarithm is the natural bridge: log(ab) = log(a) + log(b).

- **Entropy:** The logarithmic transformation turns our geometric mean of probabilities into an arithmetic mean, which we call "entropy". Defining entropy H = -Σ Pⱼ log Pⱼ as the weighted arithmetic mean of -log(p) values, which we term "surprisals". The entropy is the expected surprisal of a distribution — the expected number of bits needed to describe an outcome. Connecting this back to counting possibilities: if an event has probability 1/N, it represents one of N equally likely outcomes, and I(x) = log N. Entropy is the *additive representation* of uncertainty.

- **The Geometry of Mixing:** Understanding why mixing or blurring distributions always increases uncertainty. We develop the AM-GM inequality geometrically: for two positive numbers, the arithmetic mean is greater than or equal to the geometric mean, with equality only when the numbers are equal. The geometric proof reveals that this inequality follows from the concavity of the logarithm function (or equivalently, the convexity of -log): since log curves downward, the logarithm of an average is greater than the average of logarithms. This geometric fact has a direct interpretation for probabilities: when we mix distributions (taking weighted arithmetic means of probabilities), we move away from the geometric mean structure that naturally characterizes multiplicative systems. Since entropy is the expected value of -log(p), the convexity of -log ensures that mixing distributions increases entropy: H(weighted average) ≥ weighted average of H. The geometric reasoning shows us that this inequality holds because mixing replaces the geometric structure (which minimizes entropy for a given set of probabilities) with an arithmetic structure (which increases it). This fundamental result will be essential for understanding relative entropy in the next unit.
  - **Pedagogical note:** Here the perspective is flipped: we started with geometric mean as the natural structure for multiplicative quantities, transformed to additive space via logarithm, and now we see how operations in additive space (mixing via arithmetic means) relate back to the geometric structure through the convexity of -log.

- **Perspective:** Exploring how entropy quantifies uncertainty across diverse domains: (1) **Biological Diversity**—measuring genetic sequence diversity in populations, protein conformational entropy in folding, and ecological diversity indices; (2) **Neural Coding**—how sensory systems encode information (e.g., retinal ganglion cells transmitting visual information with varying entropy depending on scene complexity); (3) **Thermodynamics**—entropy as a measure of disorder in physical systems (the connection between information entropy and thermodynamic entropy); (4) **Data Compression**—why some files compress better than others (text with repeated patterns has lower entropy than random data); (5) **Language and Communication**—letter and word frequencies in natural languages (English has lower entropy than random letter sequences, enabling efficient encoding); (6) **Quantum Mechanics**—entropy in quantum states and measurement uncertainty. These examples show that entropy is a universal measure of "how much we don't know" across physical, biological, computational, and quantum systems.

---

## Unit 2: The Geometry of Wrongness

**Focus:** Understanding how different probability distributions correspond to different geometric structures, and measuring the "distance" between these structures to characterize the cost of using one to encode events from another.  Developing the notion of relative entropy as a measure of "wrongness" in a geometric sense.

### Topics

- **Geometric Structures of Distributions:** Each probability distribution P has its own geometric mean structure—its own natural "expected probability" that characterizes the multiplicative system. When we use a different distribution Q to describe events that actually follow P, we're using the wrong geometric structure. The relative entropy D(P||Q) = Σ P(x) log(P(x)/Q(x)) measures how far we deviate from the correct geometric structure. In the logarithmic/additive space, this becomes a measure of how much extra "information distance" we accumulate by using Q's geometric structure instead of P's.

- **The Geometry of Mismatch:** When we use Q instead of P, we're essentially measuring events against Q's geometric mean structure when they actually belong to P's geometric mean structure. The ratio P(x)/Q(x) tells us how much more (or less) likely event x is under P's structure compared to Q's structure. Taking the logarithm transforms this multiplicative ratio into an additive difference in the information space. The relative entropy is the expected value of this difference, weighted by the true distribution P.

- **Connection to Entropy:** When Q is uniform, we have D(P||Uniform) = log|X| - H(P). This shows that relative entropy measures how much we deviate from the optimal geometric structure: the uniform distribution has maximum entropy (no geometric structure), and H(P) measures how much structure P has. The difference tells us how far P is from having no structure at all. This connects relative entropy directly to the entropy we developed in Unit 1, showing it as a measure of geometric structure deviation.

- **Non-Negativity from Geometric Structure:** Using the geometry of mixing from Unit 1, we can see why D(P||Q) ≥ 0: when P ≠ Q, using Q's geometric structure to describe P's events is like mixing distributions—it moves us away from the optimal geometric structure, increasing uncertainty. The convexity of -log (which we developed geometrically) ensures this inequality holds, with equality only when P = Q (the geometric structures match). This establishes relative entropy as a measure of geometric structure distance.

- **Simple Discrete Examples:** Exploring concrete cases geometrically: comparing a fair coin (uniform geometric structure) vs. a biased coin (non-uniform geometric structure), comparing an empirical distribution (observed geometric structure) to a theoretical one (expected geometric structure), and comparing different discrete distributions (different geometric mean structures). Calculating how much the geometric structures differ in each case, and interpreting the relative entropy concretely in terms of the meaning of the "information cost" in the problem context.

- **Asymmetry and Geometric Interpretation:** Understanding why D(P||Q) ≠ D(Q||P): we're always measuring from the perspective of the true geometric structure P. The forward direction D(P||Q) asks "how far is Q's structure from P's structure?" The reverse D(Q||P) asks "how far is P's structure from Q's structure?" These are different because we're measuring deviation from different reference geometric structures. The asymmetry reflects the fact that geometric structure distance depends on which structure we treat as the reference.

- **Perspective:** Exploring contexts where predictions (Q) are compared to reality (P): (1) **Betting and Prediction Markets**—horse race odds vs. actual outcomes, sports betting, election predictions; (2) **Weather Forecasting**—predicted probability distributions vs. observed weather patterns; (3) **Machine Learning**—model predictions vs. true labels, measuring how well a classifier matches the true data distribution; (4) **Hypothesis Testing**—comparing observed data to theoretical null hypotheses (e.g., Hardy-Weinberg equilibrium in population genetics); (5) **Economics**—market model predictions vs. actual market behavior, portfolio risk models; (6) **Physics**—theoretical predictions vs. experimental observations (e.g., quantum state predictions vs. measurement outcomes). In each case, we're measuring how far the predicted geometric structure (Q) deviates from the actual geometric structure (P). Relative entropy quantifies this geometric structure mismatch across prediction, modeling, and scientific inference contexts, showing how the "wrong geometric structure" manifests as extra uncertainty in diverse domains.

---

## Unit 3: Typicality and the Law of Large Numbers

**Focus:** Understanding why, when you have many samples, nearly all outcomes fall into a "typical" set, and how this fundamental property (the AEP) connects to the Central Limit Theorem, revealing a deep symmetry in how randomness behaves.

### Topics

- **The Typical Set and AEP:** Understanding why, in large samples, nearly all outcomes fall into a "typical" subset of equal probability, even though individual sequences are rare. Proving that for *n* trials, P(x₁, ..., xₙ) ≈ 2⁻ⁿᴴ. This shows that the "volume" of likely outcomes is roughly 2ⁿᴴ. Visualizing why outcomes "clump" together in high-dimensional space, losing their individual complexity. The AEP (Asymptotic Equipartition Property) tells us that when you have many samples, almost everything behaves typically—this is a fundamental tool for understanding what happens in the limit.

- **The AEP as a Discrete Central Limit Theorem:** Establishing the symmetry between the Central Limit Theorem (CLT) and the Asymptotic Equipartition Property (AEP). The CLT tells us about the *shape* of noise (how sums of random variables distribute), while the AEP tells us about the *volume* of typical sequences (how many typical sequences there are, measured by entropy). Both describe concentration: outcomes concentrate around typical values. This reveals a deep connection between probability theory and information theory. 
 -**Pedagogical note:** Here we're viewing AEP through the lens of probability theory (AEP as a discrete CLT). In Unit 4, we'll flip this perspective and view CLT through the lens of information theory (CLT as a two-part encoding), revealing the same deep structure from the opposite direction.

- **High-Dimensional Concentration:** Visualizing why outcomes "clump" together in high-dimensional space. Understanding how the typical set occupies a tiny fraction of the total space, yet contains almost all the probability. This geometric intuition helps us understand why the AEP works: most of the "volume" of possibilities is irrelevant—only the typical set matters.

- **Large Deviations and Sanov's Theorem:** Using relative entropy (from Unit 2) to understand how far an empirical distribution can deviate from the true distribution. Sanov's theorem shows that the probability of observing an empirical distribution Q when the true distribution is P decays exponentially with n·D(Q||P). This connects the AEP (typical behavior) to large deviations (atypical behavior), showing how relative entropy quantifies how "atypical" something is.

- **Perspective:** Comparing the *shape* of noise in Quantum Physics (CLT) with the *volume* of typical sequences in a signal (AEP, measured by entropy). Exploring how concentration of measure appears across domains: statistical physics (how systems concentrate around equilibrium), machine learning (how training concentrates around typical solutions), and communication (how messages concentrate in typical sets for efficient encoding). Information theory provides the unifying framework for understanding these patterns.

---

## Unit 4: The Universal Two-Part Description: Data Compression and Optimal Encoding

**Focus:** Understanding how data compression works, how to encode data optimally, and the universal two-part description of data. This unit completes the connection between information theory and the real world by showing how the same two-part structure that simplifies physical systems (the mean and the variance) also simplifies information systems (the model and the error) and philosophical foundations (the general and the particular).

### Topics

- **Two-Part Encoding:** Introducing the idea that any data can be described as "Model Description + Residual Error." The model captures the regularities (the "universal" structure), while the error captures what's left over (the "particular" deviations). When you have enough data, this two-part structure becomes the best possible: you can't do better than describing the model and then encoding the residuals.

- **The Central Limit Theorem as a Two-Part Description:** Exploring how the CLT shows that when you have many samples, any distribution (regardless of its complexity) simplifies to a Gaussian, which is completely characterized by just two numbers: the mean (average) and variance (spread). All the other details about the distribution become irrelevant when you have enough data. This is a profound simplification: infinite complexity → two numbers. Using the AEP (from Unit 3) to understand why this happens: the typical set concentrates around the mean with a spread given by the variance.
  - **Pedagogical note:** This flips the perspective from Unit 3: there we viewed AEP as a discrete CLT (information theory through probability theory's lens). Here we view CLT as a two-part encoding (probability theory through information theory's lens). This back-and-forth across the logarithmic transformation mirrors the pattern we saw in Unit 1 (geometric mean ↔ entropy via logarithm), revealing the same deep structure from complementary angles. The same mathematical pattern appears whether we're thinking about the concentration of probability (CLT) or the encoding structure revealed by entropy (AEP), just as it appeared when we moved between multiplicative and additive perspectives in Unit 1.
  - **Pedagogical note:** The "two numbers" (mean, variance) themselves need encoding—we're describing the asymptotic structure where the model description becomes relatively small compared to the data. In the discrete case, we can be concrete about this; the rigorous continuous case involves differential entropy and gets very technical.

- **When Two Parts are Not Enough: Higher-Order Encodings:** Recognizing that the reduction to two numbers (mean and variance) and two-part encoding (model + error) are only the leading terms. When we have smaller datasets, care about convergence rates, need more precision, or have limited resources, we need higher-order terms. Just as we can refine a model by adding corrections (model + first-order error + second-order error + ...), we can refine our encoding structure. These higher-order terms correspond to higher-order entropies (conditional entropies of higher order, capturing dependencies beyond the simple model). This mirrors how distributions have more detailed statistics beyond mean and variance: the third statistic (skewness) captures asymmetry, the fourth statistic (kurtosis) captures tail behavior, and so on. The two-part/two-number structure is the main term of a deeper expansion that includes corrections. This completes the deep connection: just as physical systems have a full expansion (mean + variance + skewness + kurtosis + ...), information systems have (model + first-order error + second-order error + ...). The same mathematical structure governs both how physical systems simplify (reduction to statistics) and how information systems compress (encoding structure). When we need perfect reconstruction, we need all the statistics (all moments). As we allow approximations or have limited resources, we can use fewer bits, and the higher-order terms become less important. 

- **Perspective:** Exploring how the reduction to two numbers (and when we need more) appears across essentially all domains of knowledge: **Physics and Mathematics**—statistical mechanics (energy and entropy), quantum mechanics (expectation values and uncertainty), the Central Limit Theorem; **Biology**—population genetics (mean fitness and variance), ecological diversity indices; **Social Sciences**—demographic patterns, economic distributions, voting behavior; **Literary Theory**—the tension between general narrative structures and specific textual details, the way individual works participate in larger genres; **Philosophy**—the relationship between individual experiences and general concepts, how specific examples give rise to abstract principles, the structure of knowledge itself; **Machine Learning**—bias and variance tradeoff, the fundamental tension between model complexity and generalization; **Art and Aesthetics**—the balance between general principles of beauty and specific artistic expressions; **Data Compression**—lossy compression (JPEG, MP3) where the tradeoff between bits and quality is essential. **Rate Distortion Theory** (requiring differential entropy for rigorous treatment) provides the mathematical framework for understanding this tradeoff in continuous systems: the rate-distortion function R(D) tells us the minimum bits needed for a given distortion level, completing Shannon's theory alongside channel capacity. This universal pattern suggests a deep structural principle underlying how complexity simplifies when you have many samples—a principle that appears to be a fundamental feature of how the world (and our understanding of it) is organized. The reduction to two numbers (with higher-order corrections when needed) is not just a mathematical curiosity; it is a universal pattern that structures reality itself.

---

## Implementation Notes

- **Finitary Approach:** All concepts are introduced via discrete sums and combinatorics to keep the focus on the "logarithm of probability" rather than integral calculus. This makes the material accessible to advanced high school students while maintaining mathematical rigor.

- **Computation:** Students use computational tools (Google Colab, Python, or similar) to simulate Huffman trees, calculate entropy of natural languages vs. code, and explore information-theoretic concepts through hands-on computation.

- **Geometric Foundations:** Rather than starting with Jensen's inequality, we develop the notion of entropy geometrically, as the logarithm of the geometric mean of probabilities.  The *meaning* of this geometric mean is central to the course, as it forms a bridge between the quantitative mathematics and a qualitative "mental picture" of the problem context.  This geometric mean is the "typical probability" of the distribution, and it is the key to understanding the "typical set" and the "concentration of measure" phenomenon.

- **Calculus and Differential Entropy:** It is easy to get "lost in the weeds" of measure theory and differential entropy, but in practice the way the deep questions in these fields are always resolved is by a coherent link to the clarifying additional information of the problem context.  In this course we try to develop the intuition that leads to the correct formal results, rather than to build skill with any given formalism.  Differential entropy is introduced in a limited capacity in the "perspective" sections, but is not a central focus of the course.
 - **Technical note:** Where the *conceptual content* of the course materials seems to require integral or differential calculus, we take the approach of using discrete approximations to understand continuous concepts.  We work with finite sums and combinatorics, using these discrete cases to develop intuition about what happens in the continuous limit.  This approach is intentionally informal (we don't rigorously prove convergence), but it is a powerful way to develop the geometric and conceptual intuition that leads to the correct formal results.  The geometric intuition is essential here, as it provides a vocabulary for reasoning about continuous/infinite structures using finite/discrete tools—we can "see" what the limit should be geometrically, even when we're working with discrete approximations.
 - **Technical note:** The development of measure theory from Riemann integration can be interpreted as a refinement of the two-part encoding structure.  In Riemann integration (∫ f(x) dx), the measure is implicit and conflated with the integration process.  In measure theory (∫ f dμ), the measure μ (the "model"—how we measure) is explicitly separated from the function f (the "particular"—what we're measuring).  The "d" becomes a clear separator between the universal structure (the measure) and the particular instance (the function).  This separation makes explicit the two-part structure: measure (model) + function (particular) = integral.  This perspective also provides a clear answer to "why the codomain?": the codomain must be flexible enough (e.g., ℝ or ℂ) to encode both parts—both the structural information from the measure and the particular information from the function.  The integral's value combines both, so the codomain needs sufficient richness to represent this two-part encoding.  This is another instance of the recurring pattern where mathematical development refines our understanding by making the two-part structure explicit.

- **Linear Algebra:** Linear algebra interacts with information theory much in the way that calculus does; it is greatly-clarifying and a common "intermediate-level" encoding for many real-world applied problems; but it is not really necessary for understanding the core ideas of information theory. Precalculus covers the amount of linear algebra content that is used in the core material or in the problem sets (basic vector/matrix operations, and the notion of "dimension"); additional linear algebra content may be introduced as desired/appropriate for the particular student/class in the "perspective" sections.
 - **Technical note:** Information theory offers two complementary perspectives on the structure of linear algebra, operating at different levels.  **First perspective (the structure of a vector representation):** A basis provides an n-part encoding of a vector, where n is the dimension of the space.  Each coordinate is one "part" of the encoding—you need all n coordinates to fully specify the vector.  **Second perspective (the structure of the representation scheme itself):** Linear algebra as a whole can be seen as a two-part encoding: the choice of basis constitutes the "model" (the universal structure), and the coordinates of a vector in that basis constitute the "error" (the particular instance).  These are different levels of analysis: the first looks at how many parts are needed to encode a single vector, while the second looks at how the entire encoding scheme (basis + coordinates) has a two-part structure.  If appropriate (based on student background), the instructor may introduce a "back-and-forth" between these two perspectives, similar to the back-and-forth between multiplicative and additive perspectives in the core course material.

- **Synthesis:** The course concludes by showing that the **CLT** and **AEP** are two sides of the same "Concentration of Measure" coin—one describes the limits of physical noise, the other the limits of human (and biological) communication. A key pedagogical insight throughout the course is the recurring pattern of viewing the notion of information from different perspectives: in Unit 1, we move from geometric mean (multiplicative space) through logarithm to entropy (additive space), then back to see how mixing relates to geometric structure via convexity; in Units 3-4, we flip between viewing AEP (which lives in the logarithmic/additive space of entropy) and CLT (which lives in probability space) through each other's lenses. Both back-and-forths are fundamentally about how the logarithmic transformation connects multiplicative and additive perspectives, revealing the same deep structure from complementary angles. This demonstrates how information theory serves as a unifying meta-language: the same mathematical patterns (quantified by entropy and related measures) appear whether we're thinking about probability distributions, sequences of symbols, or the structure of knowledge itself.
